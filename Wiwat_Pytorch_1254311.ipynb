{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Breed Classifier\n",
    "\n",
    "This project is to train image classifier model to classify Cat Breeds dataset images. It has frontend as web application showing Quiz which allow user to take on guessing cat breed on the images.\n",
    "\n",
    "\n",
    "![Quiz Image](Report/quiz.png)\n",
    "\n",
    "Then after user pick answer, it will show result comparing user's with ChatGPT-4o-mini (OpenRouter API key required) and 4 Deep Learning Models. The rationale of LLM and GradCAM has been implemented to show model interpretability.\n",
    "\n",
    "![Answer Image](Report/evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets\n",
    "- Dataset is from Kaggle from 'Geno Cat Breed Image Collection' dataset (url: https://www.kaggle.com/datasets/shawngano/gano-cat-breed-image-collection)\n",
    "- Contains 15 cat breeds with 375 photos for each breed (total 5,625 photos)\n",
    "- Preprocessing step\n",
    "  - Resize to 256x256\n",
    "  - Random Crop to 224x224\n",
    "  - Random Horizontal Flip\n",
    "  - Random Rotation\n",
    "  - Convert to Tensor\n",
    "  - Normalize with mean and std of ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "### Custom Model (1 Model)\n",
    "1. CNN Model\n",
    "  - 5 Convolutional Layers \n",
    "  - 2 Fully Connected Layers\n",
    "  - 1 Dropout Layer in between the two FC Layers\n",
    "\n",
    "from src/pages.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CatBreedClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CatBreedClassifier, self).__init__()\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64 x 112 x 112\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128 x 56 x 56\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256 x 28 x 28\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 512 x 14 x 14\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # Changed from 1024 to 512\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)   # Output: 512 x 7 x 7\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size: 512 channels * 7 * 7 = 25088\n",
    "        self.fc_block = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(512 * 7 * 7, 512),  # Adjusted input size\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, 15),  # 15 breeds\n",
    "            torch.nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning (3 models)\n",
    "2. VGG 16 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_model() -> nn.Module:\n",
    "    vgg_model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    for param in vgg_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    vgg_model.classifier[6] = nn.Linear(vgg_model.classifier[6].in_features, 15)\n",
    "    return vgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ResNet 18 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model() -> nn.Module:\n",
    "    res_model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    for param in res_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    res_model.fc = nn.Linear(res_model.fc.in_features, 15)\n",
    "    return res_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. EfficientNet B2 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficient_net_model() -> nn.Module:\n",
    "    eff_model =  torchvision.models.efficientnet_b2(weights=torchvision.models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "    for param in eff_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    eff_model.classifier[1] = nn.Linear(eff_model.classifier[1].in_features, 15)\n",
    "    return eff_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "All 4 models has been trained using same hyperparamters for comparison.\n",
    "\n",
    "\n",
    "- Training Batch Size: 128\n",
    "- Learning Rate: 0.001\n",
    "- Momentum: 0.9\n",
    "- Epochs: 5\n",
    "- Loss Function: Cross Entropy Loss\n",
    "- Optimizer: SGD\n",
    "\n",
    "\n",
    "After finish training model saved to .pth for later used for inference result on Cat Quiz program.\n",
    "\n",
    "\n",
    "![pth_file](Report/pth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is from ```src\\model_train.py``` use in training all 4 models.\n",
    "\n",
    "\n",
    "- CatBreedDataSet = Custom Dataset for Kaggle Cat Breed dataset\n",
    "- get_cat_breed_dataset = function to create new catbreed dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBreedDataset(Dataset):\n",
    "    def __init__(self, image_path, class_names, transform):\n",
    "        self.image_path = image_path\n",
    "        self.class_names = class_names\n",
    "        self.class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.image_labels = []\n",
    "        self.image_tensor = []\n",
    "        for class_name in self.class_names:\n",
    "            for image_file in os.listdir(os.path.join(self.image_path, class_name)):\n",
    "                self.image_files.append(os.path.join(self.image_path, class_name, image_file))\n",
    "                self.image_labels.append(self.class_to_idx[class_name])\n",
    "                self.image_tensor.append(self.transform(Image.open(os.path.join(self.image_path, class_name, image_file)).convert('RGB')))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor = self.image_tensor[idx]\n",
    "        image_label = self.image_labels[idx]\n",
    "        return image_tensor, image_label\n",
    "\n",
    "\n",
    "def get_cat_breed_dataset(app_state, transform) -> CatBreedDataset: \n",
    "    return CatBreedDataset(app_state.image_path, app_state.class_names, transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_transformation = function to build transformation using imagenet normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transformation() -> transforms.Compose:\n",
    "    return transforms.Compose(\n",
    "        [transforms.Resize((256, 256)),\n",
    "         transforms.RandomCrop((224, 224)),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(10),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_model = function to train model and saved training accuracy/loss and validation accuracy/loss for later display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(app_state,model: nn.Module, model_name: str) -> tuple[list[float], list[float], list[float], list[float], float]:\n",
    "    print(f\"Training model on {app_state.device}\")\n",
    "    transform = get_transformation()\n",
    "     \n",
    "    print(f\"Loading dataset\")\n",
    "    dataset = get_cat_breed_dataset(app_state, transform)\n",
    "    \n",
    "    # Split dataset into train and test sets (80% train, 20% test)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create data loaders for both sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f\"Dataset split: {train_size} training samples, {test_size} test samples\")\n",
    "    \n",
    "    print(f\"Loading model\")\n",
    "    model.to(app_state.device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    training_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_loss = []\n",
    "    validation_accuracy = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(app_state.device)\n",
    "            labels = labels.to(app_state.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}\")\n",
    "            \n",
    "        training_loss.append(running_loss / len(train_loader))\n",
    "        training_accuracy.append(correct / total)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(app_state.device)\n",
    "                labels = labels.to(app_state.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0) \n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            validation_loss.append(running_loss / len(test_loader))\n",
    "            validation_accuracy.append(correct / total)\n",
    "            \n",
    "        print(f\"Accuracy of the network on the {total} test images: {100 * correct / total}%\")\n",
    "    print(f\"Finished Training\")\n",
    "    print(f\"Loss: {running_loss / len(train_loader)}\")\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Saving model to {model_name}\")\n",
    "    scripted_model = torch.jit.script(model)\n",
    "    torch.jit.save(scripted_model, model_name)\n",
    "    \n",
    "    return training_loss, training_accuracy, validation_loss, validation_accuracy, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Wiwat_Pytorch_1254311.ipynb to pdf\n",
      "[NbConvertApp] Writing 51772 bytes to notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
      "[NbConvertApp] WARNING | b had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 3006961 bytes to Wiwat_Pytorch_1254311.pdf\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to pdf Wiwat_Pytorch_1254311.ipynb --template latex_authentic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Interpretability \n",
    "\n",
    "- GradCAM has been implemented for each models.\n",
    "  - Color will gradient from Blue(0%) -> Cyan(25%) -> Green(50%) -> Yellow(75%) -> Red(100%) impact to features that CNN layer consider.\n",
    "\n",
    "- Code below show GradCAM implemmented on CNN Layer selection for each model from src\\model_train.py\n",
    "\n",
    "```py\n",
    "    target_layer = None\n",
    "    \n",
    "    if model_name == Models.ClassificationModel:\n",
    "        # For custom CNN, use the last conv layer in conv_block\n",
    "        target_layer = original_model.conv_block[-3]  # Get the last Conv2d before the final MaxPool\n",
    "    elif model_name == Models.ResNetModel:\n",
    "        # For ResNet, use the last layer in layer4\n",
    "        target_layer = original_model.layer4[-1].conv2\n",
    "    elif model_name == Models.EfficientNetModel:\n",
    "        # For EfficientNet, use the last conv layer in features \n",
    "        target_layer = original_model.features[-1][0] \n",
    "    elif model_name == Models.VGGModel:\n",
    "        # For VGG, use the last conv layer in features\n",
    "        for module in original_model.features:\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                target_layer = module\n",
    "                \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
